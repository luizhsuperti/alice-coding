{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daly Weights, inflow and outflow of ICD-10 - dalys: Panel creation\n",
    "\n",
    "This script runs a `sql` code to create a panel data, based on the `case_records_current` table, where we have each `ICD-10` member change of status, and the link between the status change with change in daly weights. \n",
    "The calculation involves a lot of cleaning,   such as desconsidering impetous changes of status or *cancelled* icds status as the first status input, etc, we leave that for the sql code, and here is just the script to \n",
    "run it and export to the redshift.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    - run this notebook to upload to Redshift;\n",
    "    \n",
    "    - use the new panel to make calculations, new tables, dashes...\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection to redshift\n",
    "from redshift_import_export.interfaces.data_interactor import DataInteractor\n",
    "di = DataInteractor()\n",
    "import requests\n",
    "\n",
    "\n",
    "#standard packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#time packages\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'WITH \n---get the icd-10 registries at Alice\ncdc_df AS (\n    SELECT \n        crc.person_internal_code AS member_internal_code,\n        crc.case_id,\n        crc.sex,\n        crc.age,\n        crc.disease_code_value,\n        crc.status,\n        crc.added_at::timestamp as added_at,\n        crc.severity\n    FROM\n        \"public\".\"case_record_current\" AS crc\n    WHERE\n        crc.status <> 'PENDING'\n        AND crc.status <> 'CONTEMPLATING'\n        AND crc.disease_code_type = 'CID_10'\n),\ncdc_df_distinct as(\n    SELECT DISTINCT\n        cdc_df.member_internal_code,\n        cdc_df.case_id,\n        cdc_df.added_at::timestamp as added_at,\n        cdc_df.disease_code_value,\n        cdc_df.status,\n        cdc_df.severity\n    FROM \n        cdc_df\n    \n    ), \n--FROM cte to cte6, we build a table that, for each case_id, we get\n-- the order of updates. That way, we can compare a status change to\n-- the previous status change and see if it is an improvement or not.\ncte AS(\n    SELECT\n        DISTINCT cdc_df.member_internal_code,\n        cdc_df.case_id,\n        cdc_df.added_at :: timestamp AS added_at,\n        ROW_NUMBER() OVER (PARTITION BY member_internal_code,case_id ORDER BY added_at) AS sort\n    FROM\n        cdc_df\n),\n-- Here we don't want to get weird updates, such as those in between 1 minute.\ncte2 AS -- Let's find the status updt of more than 1hour of difference betw\n(\n    SELECT\n        cte.*,\n        CASE\n            WHEN DATEDIFF('hour', cte_1.added_at, cte.added_at) <= 1 THEN 0\n            ELSE 1\n        END AS GrpType\n    FROM\n        cte\n    LEFT OUTER JOIN cte AS cte_1 \n        ON cte.sort = cte_1.sort + 1\n        AND cte.member_internal_code = cte_1.member_internal_code\n        AND cte.case_id = cte_1.case_id\n),\ncte3 AS -- assign a Sequence id for each update within each case_id.\n(\n    SELECT\n        member_internal_code,\n        case_id,\n        GrpType,\n        added_at,\n        ROW_NUMBER() OVER(PARTITION BY member_internal_code,case_id ORDER BY added_at) SeqId\n    FROM\n        cte2\n    WHERE\n        GrpType = 1\n),\ncte4 AS -- find the timestamp range per sequence_id. \n(    SELECT\n        cte3.*,\n        cte_2.added_at AS TS_to\n    FROM\n        cte3\n    LEFT OUTER JOIN cte3 AS cte_2 \n        ON cte3.SeqId = cte_2.SeqId -1\n        AND cte3.member_internal_code = cte_2.member_internal_code\n        AND cte3.case_id = cte_2.case_id\n),\ncte5 AS(\n    -- we get the correct ordering of updates (seq_id), with each\n    -- other update within 1 hour is assigned to a seq_id, and a seq_order within that seq_id.\n    SELECT\n        t.member_internal_code,\n        t.case_id,\n        t.disease_code_value,\n        t.status,\n        t.severity,\n        t.added_at,\n        cte4.SeqId,\n        ROW_NUMBER() OVER(PARTITION BY t.member_internal_code, t.case_id, cte4.SeqId ORDER BY t.added_at) AS SeqOrder\n    FROM\n        cte4\n    INNER JOIN cdc_df_distinct AS t ON t.member_internal_code = cte4.member_internal_code\n        AND t.case_id = cte4.case_id\n        AND t.added_at >= cte4.added_at\n        AND (t.added_at < cte4.TS_to OR cte4.TS_to IS NULL)\n),\n-- cte6-cte7:Here we will only get the seq_order per seq_id. That way, we only get the update\n-- that the HP thinks is the most correct\ncte6 AS(\n    SELECT\n        cte5.member_internal_code,\n        cte5.case_id,\n        cte5.disease_code_value,\n        cte5.status,\n        cte5.severity,\n        cte5.added_at,\n        cte5.SeqId AS seq_id,\n        cte5.SeqOrder AS seq_order,\n        MAX(cte5.SeqOrder) OVER (PARTITION BY member_internal_code, case_id, SeqId) AS max_seq_order\n    FROM\n        cte5\n),\n\ncte7 AS ( --consider 'double' cids subcategories, cid_code1/cid_code2 as the cid category of the first one.\n    SELECT\n        cte6.member_internal_code,\n        cte6.case_id,\n        CASE\n            WHEN cte6.disease_code_value LIKE '%/%' THEN SUBSTRING(cte6.disease_code_value, 1, 3)\n            ELSE cte6.disease_code_value\n        END AS disease_code_value,\n        cte6.status,\n        cte6.severity,\n        cte6.added_at,\n        cte6.seq_id\n    FROM\n        cte6\n    WHERE\n        seq_order = max_seq_order\n),\n\n--from df1 to df7, we clean the table, as well as applying functions to get the change in dalys, from one\n-- status change to another.\n\ndf1 AS(-- to get the change in dalys, let's get the previous status, cid, etc. but let's not consider\n        -- entries that were cancelled right off the bat.\n    SELECT\n        cte7.member_internal_code,\n        cte7.case_id,\n        cte7.disease_code_value,\n        cte7.status,\n        cte7.severity,\n        LAG(cte7.disease_code_value) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS disease_code_value_lag1,\n        LAG(cte7.status) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS status_lag1,\n        LAG(cte7.severity) OVER ( PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS severity_lag1,\n        cte7.added_at,\n        cte7.seq_id\n    FROM\n        cte7\n    WHERE\n        cte7.seq_id > 1\n        OR (cte7.seq_id = 1 AND STATUS !='CANCELLED' AND severity != 'INACTIVE')\n),\n\ndf2 AS(--since we're desconsidering cancellations as the first entry, we need to create a new ordering.\n        -- Additionally, we will not consider changes that did update anything, that is, the status and severity\n        -- were the same as before. \n    SELECT\n        df1.member_internal_code,\n        df1.case_id,\n        df1.seq_id,\n        ROW_NUMBER() OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS seq_id_v2,\n        df1.disease_code_value,\n        df1.status,\n        df1.severity,\n        df1.added_at\n    FROM\n        df1\n    WHERE\n        (disease_code_value_lag1 IS NULL AND status_lag1 IS NULL AND severity_lag1 IS NULL)\n         OR (disease_code_value_lag1 != disease_code_value OR  status!=status_lag1 OR severity != severity_lag1)\n),\ndf3 AS(--renaming the new sequence id (v2) to the original\n    SELECT\n        df2.member_internal_code,\n        df2.case_id,\n        df2.seq_id_v2 AS seq_id,\n        df2.disease_code_value AS cid,\n        df2.status AS cid_status,\n        df2.severity AS cid_sev,\n        df2.added_at\n    FROM\n        df2\n),\ndf4 AS( --finally, let's retrieve the DW, disability weights for daly. notice that cancel/inactive = 0.\n    SELECT\n        df3.member_internal_code,\n        df3.case_id,\n        df3.cid,\n        df3.seq_id,\n        df3.cid_status,\n        df3.cid_sev,\n        df3.added_at,\n        CASE \n            WHEN cid_status = 'CANCELLED' OR cid_sev = 'INACTIVE' THEN 0\n            ELSE COALESCE(rd.daly_weight_lower, 0) \n        END AS daly_weight_lb\n    FROM\n        df3\n        LEFT JOIN \"restricted_datascience\".\"daly_weights_for_alice\" AS rd ON df3.cid = rd.cid\n        AND LOWER(df3.cid_sev) = LOWER(rd.severity)\n),\ndf5 AS(-- as in df1, we need to get the previous status, severity, daly...\n    SELECT\n        df4.member_internal_code,\n        df4.case_id,\n        df4.cid,\n        df4.seq_id,\n        df4.cid_status,\n        df4.cid_sev,\n        df4.daly_weight_lb,\n        df4.added_at,\n        LAG(df4.cid) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS cid_lag1,\n        LAG(df4.cid_status) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS cid_status_lag1,\n        LAG(df4.cid_sev) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS cid_sev_lag1,\n        LAG(df4.daly_weight_lb) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS daly_weight_lag1\n    FROM\n        df4\n),\ndf6 AS( --create new column, delta_daly, that is just the difference between DW from the current status change to the old one.\n    SELECT\n        df5.member_internal_code,\n        df5.case_id,\n        df5.seq_id,\n        df5.cid,\n        df5.cid_status,\n        df5.cid_sev,\n        df5.added_at,\n        df5.cid_lag1,\n        df5.cid_status_lag1,\n        df5.cid_sev_lag1,\n        df5.daly_weight_lb,\n        df5.daly_weight_lag1,\n        df5.daly_weight_lb - df5.daly_weight_lag1 AS delta_daly\n    FROM\n        df5\n),\ndf7 AS(--Let's reorganize the dalys weights to create an inflow/outflow perspective. e.g., if a member \n        -- is entering, then the inflow = DW. if order>1 and it has a delta<0, that means it was 'cured'\n        -- and we have and outflow. if delta>0, that means it got worse, and we have an inflow. The\n        -- other categories are for cids that we did not input yet, so we consider= zero. Or, if entered\n        -- right off the bat as cancel/inactive, we should consider the DW as outflow, although it should\n        -- not happen since we desconsidered those at previous steps.\n    SELECT\n        df6.member_internal_code,\n        df6.case_id,\n        df6.seq_id,\n        df6.cid,\n        df6.cid_status,\n        df6.cid_sev,\n        df6.added_at,\n        df6.daly_weight_lb,\n        df6.delta_daly,\n        CASE\n            WHEN seq_id = 1 AND (cid_status != 'CANCELLED' AND cid_sev != 'INACTIVE') THEN daly_weight\n            WHEN seq_id > 1 AND delta_daly > 0 THEN delta_daly\n            ELSE 0\n        END AS daly_inflow,\n        CASE \n            WHEN seq_id = 1 AND (cid_status = 'CANCELLED' OR cid_sev = 'INACTIVE') THEN daly_weight\n            WHEN seq_id > 1 AND delta_daly < 0 THEN -delta_daly\n            WHEN seq_id > 1 AND (cid_status = 'CANCELLED' OR cid_sev = 'INACTIVE') THEN daly_weight\n            ELSE 0\n        END AS daly_outflow\n    FROM\n        df6\n),\n\nfinal_panel_df AS(--now, let's bring the cid categories(as the health squad categorize) and chapters (icd-original)\nSELECT\n    df7.member_internal_code,\n    df7.case_id,\n    df7.seq_id,\n    df7.cid,\n    base_chap.capitulo AS cid_cap,\n    alice_categories_df.alice_category AS cid_alice_cat,\n    df7.cid_status,\n    df7.cid_sev,\n    df7.added_at,\n    df7.daly_weight_lb,\n    df7.delta_daly,\n    df7.daly_inflow,\n    df7.daly_outflow\nFROM\n    df7\n    LEFT JOIN \"restricted_health_information\".\"base_completa_cid\" AS base_chap ON df7.cid = base_chap.cid\n    LEFT JOIN \"restricted_datascience\".\"alice_cid_categories\" AS alice_categories_df ON df7.cid = alice_categories_df.cid\nORDER BY\n    member_internal_code,\n    case_id,\n    seq_id DESC\n)\n\nSELECT * FROM final_panel_df': column \"daly_weight\" does not exist in df6\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedColumn\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/sql.py:2023\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2023\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2024\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mUndefinedColumn\u001b[0m: column \"daly_weight\" does not exist in df6\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/luiz.superti/Documents/GitHub/alice-coding/redshift_import_export/notebooks/04_daly_lower_bounds_weights_member_cid_table.ipynb Celda 5\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/luiz.superti/Documents/GitHub/alice-coding/redshift_import_export/notebooks/04_daly_lower_bounds_weights_member_cid_table.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m di\u001b[39m.\u001b[39;49mredshift\u001b[39m.\u001b[39;49mrun_sql_query(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), os\u001b[39m.\u001b[39;49mpardir)) \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/data/queries/daly_lower_bounds_weights_member_cid_panel_table.sql\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/luiz.superti/Documents/GitHub/alice-coding/redshift_import_export/notebooks/04_daly_lower_bounds_weights_member_cid_table.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/alice-coding/redshift_import_export/src/redshift_import_export/interfaces/data_interactor.py:74\u001b[0m, in \u001b[0;36mWarehouseDataInteractor.run_sql_query\u001b[0;34m(self, sql_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(get_queries_path(sql_path), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[1;32m     69\u001b[0m conn \u001b[39m=\u001b[39m psycopg2\u001b[39m.\u001b[39mconnect(host\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint,\n\u001b[1;32m     70\u001b[0m                         port\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport,\n\u001b[1;32m     71\u001b[0m                         database\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbname,\n\u001b[1;32m     72\u001b[0m                         user\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser,\n\u001b[1;32m     73\u001b[0m                         password\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken)\n\u001b[0;32m---> 74\u001b[0m data \u001b[39m=\u001b[39m sqlio\u001b[39m.\u001b[39;49mread_sql_query(query, conn)\n\u001b[1;32m     75\u001b[0m conn\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/sql.py:400\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    401\u001b[0m     sql,\n\u001b[1;32m    402\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    403\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    404\u001b[0m     coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    405\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    406\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    407\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    408\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/sql.py:2083\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[1;32m   2072\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2073\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2079\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2080\u001b[0m ):\n\u001b[1;32m   2082\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2083\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   2084\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[1;32m   2086\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/sql.py:2035\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2032\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[1;32m   2034\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2035\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'WITH \n---get the icd-10 registries at Alice\ncdc_df AS (\n    SELECT \n        crc.person_internal_code AS member_internal_code,\n        crc.case_id,\n        crc.sex,\n        crc.age,\n        crc.disease_code_value,\n        crc.status,\n        crc.added_at::timestamp as added_at,\n        crc.severity\n    FROM\n        \"public\".\"case_record_current\" AS crc\n    WHERE\n        crc.status <> 'PENDING'\n        AND crc.status <> 'CONTEMPLATING'\n        AND crc.disease_code_type = 'CID_10'\n),\ncdc_df_distinct as(\n    SELECT DISTINCT\n        cdc_df.member_internal_code,\n        cdc_df.case_id,\n        cdc_df.added_at::timestamp as added_at,\n        cdc_df.disease_code_value,\n        cdc_df.status,\n        cdc_df.severity\n    FROM \n        cdc_df\n    \n    ), \n--FROM cte to cte6, we build a table that, for each case_id, we get\n-- the order of updates. That way, we can compare a status change to\n-- the previous status change and see if it is an improvement or not.\ncte AS(\n    SELECT\n        DISTINCT cdc_df.member_internal_code,\n        cdc_df.case_id,\n        cdc_df.added_at :: timestamp AS added_at,\n        ROW_NUMBER() OVER (PARTITION BY member_internal_code,case_id ORDER BY added_at) AS sort\n    FROM\n        cdc_df\n),\n-- Here we don't want to get weird updates, such as those in between 1 minute.\ncte2 AS -- Let's find the status updt of more than 1hour of difference betw\n(\n    SELECT\n        cte.*,\n        CASE\n            WHEN DATEDIFF('hour', cte_1.added_at, cte.added_at) <= 1 THEN 0\n            ELSE 1\n        END AS GrpType\n    FROM\n        cte\n    LEFT OUTER JOIN cte AS cte_1 \n        ON cte.sort = cte_1.sort + 1\n        AND cte.member_internal_code = cte_1.member_internal_code\n        AND cte.case_id = cte_1.case_id\n),\ncte3 AS -- assign a Sequence id for each update within each case_id.\n(\n    SELECT\n        member_internal_code,\n        case_id,\n        GrpType,\n        added_at,\n        ROW_NUMBER() OVER(PARTITION BY member_internal_code,case_id ORDER BY added_at) SeqId\n    FROM\n        cte2\n    WHERE\n        GrpType = 1\n),\ncte4 AS -- find the timestamp range per sequence_id. \n(    SELECT\n        cte3.*,\n        cte_2.added_at AS TS_to\n    FROM\n        cte3\n    LEFT OUTER JOIN cte3 AS cte_2 \n        ON cte3.SeqId = cte_2.SeqId -1\n        AND cte3.member_internal_code = cte_2.member_internal_code\n        AND cte3.case_id = cte_2.case_id\n),\ncte5 AS(\n    -- we get the correct ordering of updates (seq_id), with each\n    -- other update within 1 hour is assigned to a seq_id, and a seq_order within that seq_id.\n    SELECT\n        t.member_internal_code,\n        t.case_id,\n        t.disease_code_value,\n        t.status,\n        t.severity,\n        t.added_at,\n        cte4.SeqId,\n        ROW_NUMBER() OVER(PARTITION BY t.member_internal_code, t.case_id, cte4.SeqId ORDER BY t.added_at) AS SeqOrder\n    FROM\n        cte4\n    INNER JOIN cdc_df_distinct AS t ON t.member_internal_code = cte4.member_internal_code\n        AND t.case_id = cte4.case_id\n        AND t.added_at >= cte4.added_at\n        AND (t.added_at < cte4.TS_to OR cte4.TS_to IS NULL)\n),\n-- cte6-cte7:Here we will only get the seq_order per seq_id. That way, we only get the update\n-- that the HP thinks is the most correct\ncte6 AS(\n    SELECT\n        cte5.member_internal_code,\n        cte5.case_id,\n        cte5.disease_code_value,\n        cte5.status,\n        cte5.severity,\n        cte5.added_at,\n        cte5.SeqId AS seq_id,\n        cte5.SeqOrder AS seq_order,\n        MAX(cte5.SeqOrder) OVER (PARTITION BY member_internal_code, case_id, SeqId) AS max_seq_order\n    FROM\n        cte5\n),\n\ncte7 AS ( --consider 'double' cids subcategories, cid_code1/cid_code2 as the cid category of the first one.\n    SELECT\n        cte6.member_internal_code,\n        cte6.case_id,\n        CASE\n            WHEN cte6.disease_code_value LIKE '%/%' THEN SUBSTRING(cte6.disease_code_value, 1, 3)\n            ELSE cte6.disease_code_value\n        END AS disease_code_value,\n        cte6.status,\n        cte6.severity,\n        cte6.added_at,\n        cte6.seq_id\n    FROM\n        cte6\n    WHERE\n        seq_order = max_seq_order\n),\n\n--from df1 to df7, we clean the table, as well as applying functions to get the change in dalys, from one\n-- status change to another.\n\ndf1 AS(-- to get the change in dalys, let's get the previous status, cid, etc. but let's not consider\n        -- entries that were cancelled right off the bat.\n    SELECT\n        cte7.member_internal_code,\n        cte7.case_id,\n        cte7.disease_code_value,\n        cte7.status,\n        cte7.severity,\n        LAG(cte7.disease_code_value) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS disease_code_value_lag1,\n        LAG(cte7.status) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS status_lag1,\n        LAG(cte7.severity) OVER ( PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS severity_lag1,\n        cte7.added_at,\n        cte7.seq_id\n    FROM\n        cte7\n    WHERE\n        cte7.seq_id > 1\n        OR (cte7.seq_id = 1 AND STATUS !='CANCELLED' AND severity != 'INACTIVE')\n),\n\ndf2 AS(--since we're desconsidering cancellations as the first entry, we need to create a new ordering.\n        -- Additionally, we will not consider changes that did update anything, that is, the status and severity\n        -- were the same as before. \n    SELECT\n        df1.member_internal_code,\n        df1.case_id,\n        df1.seq_id,\n        ROW_NUMBER() OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS seq_id_v2,\n        df1.disease_code_value,\n        df1.status,\n        df1.severity,\n        df1.added_at\n    FROM\n        df1\n    WHERE\n        (disease_code_value_lag1 IS NULL AND status_lag1 IS NULL AND severity_lag1 IS NULL)\n         OR (disease_code_value_lag1 != disease_code_value OR  status!=status_lag1 OR severity != severity_lag1)\n),\ndf3 AS(--renaming the new sequence id (v2) to the original\n    SELECT\n        df2.member_internal_code,\n        df2.case_id,\n        df2.seq_id_v2 AS seq_id,\n        df2.disease_code_value AS cid,\n        df2.status AS cid_status,\n        df2.severity AS cid_sev,\n        df2.added_at\n    FROM\n        df2\n),\ndf4 AS( --finally, let's retrieve the DW, disability weights for daly. notice that cancel/inactive = 0.\n    SELECT\n        df3.member_internal_code,\n        df3.case_id,\n        df3.cid,\n        df3.seq_id,\n        df3.cid_status,\n        df3.cid_sev,\n        df3.added_at,\n        CASE \n            WHEN cid_status = 'CANCELLED' OR cid_sev = 'INACTIVE' THEN 0\n            ELSE COALESCE(rd.daly_weight_lower, 0) \n        END AS daly_weight_lb\n    FROM\n        df3\n        LEFT JOIN \"restricted_datascience\".\"daly_weights_for_alice\" AS rd ON df3.cid = rd.cid\n        AND LOWER(df3.cid_sev) = LOWER(rd.severity)\n),\ndf5 AS(-- as in df1, we need to get the previous status, severity, daly...\n    SELECT\n        df4.member_internal_code,\n        df4.case_id,\n        df4.cid,\n        df4.seq_id,\n        df4.cid_status,\n        df4.cid_sev,\n        df4.daly_weight_lb,\n        df4.added_at,\n        LAG(df4.cid) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS cid_lag1,\n        LAG(df4.cid_status) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS cid_status_lag1,\n        LAG(df4.cid_sev) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS cid_sev_lag1,\n        LAG(df4.daly_weight_lb) OVER (PARTITION BY member_internal_code,case_id ORDER BY seq_id ASC) AS daly_weight_lag1\n    FROM\n        df4\n),\ndf6 AS( --create new column, delta_daly, that is just the difference between DW from the current status change to the old one.\n    SELECT\n        df5.member_internal_code,\n        df5.case_id,\n        df5.seq_id,\n        df5.cid,\n        df5.cid_status,\n        df5.cid_sev,\n        df5.added_at,\n        df5.cid_lag1,\n        df5.cid_status_lag1,\n        df5.cid_sev_lag1,\n        df5.daly_weight_lb,\n        df5.daly_weight_lag1,\n        df5.daly_weight_lb - df5.daly_weight_lag1 AS delta_daly\n    FROM\n        df5\n),\ndf7 AS(--Let's reorganize the dalys weights to create an inflow/outflow perspective. e.g., if a member \n        -- is entering, then the inflow = DW. if order>1 and it has a delta<0, that means it was 'cured'\n        -- and we have and outflow. if delta>0, that means it got worse, and we have an inflow. The\n        -- other categories are for cids that we did not input yet, so we consider= zero. Or, if entered\n        -- right off the bat as cancel/inactive, we should consider the DW as outflow, although it should\n        -- not happen since we desconsidered those at previous steps.\n    SELECT\n        df6.member_internal_code,\n        df6.case_id,\n        df6.seq_id,\n        df6.cid,\n        df6.cid_status,\n        df6.cid_sev,\n        df6.added_at,\n        df6.daly_weight_lb,\n        df6.delta_daly,\n        CASE\n            WHEN seq_id = 1 AND (cid_status != 'CANCELLED' AND cid_sev != 'INACTIVE') THEN daly_weight\n            WHEN seq_id > 1 AND delta_daly > 0 THEN delta_daly\n            ELSE 0\n        END AS daly_inflow,\n        CASE \n            WHEN seq_id = 1 AND (cid_status = 'CANCELLED' OR cid_sev = 'INACTIVE') THEN daly_weight\n            WHEN seq_id > 1 AND delta_daly < 0 THEN -delta_daly\n            WHEN seq_id > 1 AND (cid_status = 'CANCELLED' OR cid_sev = 'INACTIVE') THEN daly_weight\n            ELSE 0\n        END AS daly_outflow\n    FROM\n        df6\n),\n\nfinal_panel_df AS(--now, let's bring the cid categories(as the health squad categorize) and chapters (icd-original)\nSELECT\n    df7.member_internal_code,\n    df7.case_id,\n    df7.seq_id,\n    df7.cid,\n    base_chap.capitulo AS cid_cap,\n    alice_categories_df.alice_category AS cid_alice_cat,\n    df7.cid_status,\n    df7.cid_sev,\n    df7.added_at,\n    df7.daly_weight_lb,\n    df7.delta_daly,\n    df7.daly_inflow,\n    df7.daly_outflow\nFROM\n    df7\n    LEFT JOIN \"restricted_health_information\".\"base_completa_cid\" AS base_chap ON df7.cid = base_chap.cid\n    LEFT JOIN \"restricted_datascience\".\"alice_cid_categories\" AS alice_categories_df ON df7.cid = alice_categories_df.cid\nORDER BY\n    member_internal_code,\n    case_id,\n    seq_id DESC\n)\n\nSELECT * FROM final_panel_df': column \"daly_weight\" does not exist in df6\n"
     ]
    }
   ],
   "source": [
    "df = di.redshift.run_sql_query(os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + '/data/queries/daly_lower_bounds_weights_member_cid_panel_table.sql')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di.redshift.insert_table(df, table_name='daly_lower_bounds_weights_member_cid_panel_table', if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
